




# Modèles linéaires (rappel) 

$Y$ est expliquée (modélisée) par  les variables explicatives $X= (X_1,X_2,...,X_p)$.

Si $p=1$, c’est une fonction affine de X.

$Y$ est quantitative, $X$ peut être quantitative (regression lineaire) ou qualitative (ANOVA).
 
Modèle est noté : 

$$\mathbb{E}(Y)  = \beta X = \beta_0 + \beta_1X_1 + ... + \beta_pX_p$$

avec $\beta=(\beta_0, \beta_1, ..., \beta_p)$ estimé par les moindres carrées :

$$\beta = argmin(\sum_{i=1}^{n} {(y_i - \beta_0 - \beta_1x_{1,i} - ... - \beta_px_{p,i})^2})$$ 

La valeur ajustée pour l’individu $i$ est noté :
$\widehat y_i  = \beta X_i = \beta_0 + \beta_1 X_{1,i} + ... + \beta_pX_{p,i}$, 
ainsi : 

$$\beta = argmin(\sum_{i=1}^{n} {(y_i - \widehat y_i)^2})$$ 


Les résidus sont notés  $e_i = y_i -\widehat{y}_i$, soit : 

$$\beta = argmin(\sum_{i=1}^{n} {e_i^2})$$ 


Le pourcentage de variance expliquée est noté :
$$R^2 = \frac{\sum_{i}{(y_i -\overline{y}_i)^2 - e_i^2}}{\sum_{i=1}^{n}{(y_i -\overline{y}_i)^2}}$$





```{r message=FALSE, warning=FALSE}
d = MASS::cats
m = lm(Bwt~Hwt, d)
layout(matrix(1:2, 1), respect=TRUE)
plot(
  m$model$Hwt, m$model$Bwt,
  ylab="Body weight (Bwt)", xlab="Heart weight (Hwt)",
  main="Regression lineaire (Bwt~Hwt)"
)
abline(m, col=2)
arrows(d$Hwt, d$Bwt, d$Hwt, d$Bwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)

i = which(d$Hwt==17.2)
points(m$model[i,]$Hwt, m$model[i,]$Bwt,  pch=16)
points(m$model[i,]$Hwt, d[i,]$Bwt-m$residuals[i],  pch=16)
arrows(d[i,]$Hwt, d[i,]$Bwt, d[i,]$Hwt, d[i,]$Bwt-m$residuals[i], col=4, lwd=2, length=0.1)
text(m$model[i,]$Hwt, m$model[i,]$Bwt, "yi", pos=1, cex=1.3)
text(m$model[i,]$Hwt, d[i,]$Bwt-m$residuals[i]/2,  "ei", pos=4, cex=1.3)
text(m$model[i,]$Hwt, d[i,]$Bwt-m$residuals[i],  "^yi", pos=3, cex=1.3)
legend("bottomright",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)


boxplot(Bwt~Sex, d, main="ANOVA (Bwt~Sex)", xlab="Sex", ylab="Bwt", outline=FALSE, ylim=range(d$Bwt))
m = lm(Bwt~Sex, d)
m$coefficients
abline(h=m$coefficients[[1]] + m$coefficients[[2]], col=2)
abline(h=m$coefficients[[1]], col=2, lty=2)

x = jitter(as.numeric(d$Sex), 1.5)
points(x, d$Bwt)
arrows(x, d$Bwt, x, d$Bwt-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("b_0", "b_0+b_1", "residuals"), col=c(2,2,4), lty=c(2,1,1), cex=.8)

# t.test
# 1 null hypothesis
# H_0: p_h==_f
# H_1: p_h!=p_f
# 2 risk
# alpha = 5%
# 3 test hypothesis
# residuals~N?
shapiro.test(d[d$Sex=="F",]$Bwt)
shapiro.test(d[d$Sex=="M",]$Bwt)
# -> OK
# 4 t.test
t.test(d[d$Sex=="F",]$Bwt, d[d$Sex=="M",]$Bwt)
anova(m)
# 5 conclusion
```




















# Variable explicative quantitative

On considère dans cette section x une variable quantitaive et on cherche à modéliser

$Y ~ x$


### Tentative avec la regression linéaire


Considérons un second modèle dans lequel $\pi(x) = \beta_0 + \beta_1 x$ :

$$\mathbb{P}(Y=1|x)=\pi (x) = \beta_0 + \beta_1 x$$

Problème $\pi(x)$ prends des valeurs négatives et des valeurs supérieur à 1

```{r}
d = MASS::cats

layout(1, respect=TRUE)
s = as.numeric(d$Sex) - 1
plot(d$Bwt, s, main="Regression linéaire (Sex~Bwt)", xlab="Bwt", ylab="Sex", ylim=c(0,1.5))
m = lm(s~d$Bwt)
abline(m, col=2, lwd=2)
# abline(h=0.5, col="grey", lwd=2)
arrows(d$Bwt, s, d$Bwt, s-m$residuals, col=adjustcolor(4, alpha.f=0.2), length=0.1, lwd=2)
legend("bottomright",c("regression line", "residuals"), col=c(2,4), lty=1, cex=0.6)
```


### La fonction logit

On introduit donc la fonction *logit*






\begin{eqnarray}
\text{logit: } ]0,1[ &\rightarrow& \mathbb{R}                  &\qquad& \lim_{x\to0} logit(x) &=& -\infty  \hspace{12cm}\\
                   x &\rightarrow& logit(x)=log(\frac{x}{1-x}) &\qquad& \lim_{x\to1} logit(x) &=& +\infty  \hspace{12cm}\\
\end{eqnarray}





\begin{eqnarray}
\hspace{12cm} \text{logit$^{-1}$: } \mathbb{R} &\rightarrow& ]0,1[                            &\qquad& \lim_{x\to-\infty} logit^{-1}(x) &=& 0\\
\hspace{12cm}                                x &\rightarrow& logit^{-1}(x)=\frac{1}{1+e^{-x}} &\qquad& \lim_{x\to+\infty} logit^{-1}(x) &=& 1\\
\end{eqnarray}
















```{r}
layout(matrix(1:2, 1), respect=TRUE)
x = 0:100/100
plot(x, log(x/(1-x)), main="logit", type="l")
x = seq(-4, 4,  length.out=100)
plot(x, 1 / (1+exp(-x)), main="logit^-1", type="l")
```

Et on considérele le modèle logistique dans lequel $\pi(x) = logit^{-1}(\beta_0 + \beta_1 x)$ :

$$\mathbb{P}(Y=1|x) = \pi(x) = logit^{-1}(\beta_0 + \beta_1 x)$$ 


```{r}
layout(1, respect=TRUE)
plot(d$Bwt, s, main="Sex~Bwt", xlab="Bwt", ylab="Sex")
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
logitinv = function(x) 1/(1 + exp(-x))
x = seq(min(d$Bwt), max(d$Bwt), length.out=30)
lines(x, logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x), col=2, lwd=2)
py1x = function(t,m) {
  x = m$coefficients[[1]] + m$coefficients[[2]]*t
  1/(1 + exp(-x))
}
arrows(d$Bwt, s, d$Bwt, py1x(d$Bwt,m), col=adjustcolor(4, alpha.f=0.2), length=0.05, lwd=3)
legend("bottomright", c(expression(paste("P(Y=1|x)=", pi, "(x)=", logit^-1, "(", beta , "x)")), expression("1 - P(Y=y_i|X=x_i)")), col=c(2,4), lty=1, cex=0.6)
```


On **généralise** aisement le modèle logistique pour des variables explicatives **multivariées** $X=(X_1,...,X_p))$, $X_i$ pouvant être **qualitatives** ou **quantitaves**, $\beta=(\beta_0, \beta_1, ..., \beta_p)$ : 


$$\mathbb{P}(Y=1|X) = \pi(X) = logit^{-1}(\beta X)$$ 














## Mesures d'intérêt

On définit les **Odds** :

$$Odds(X) = \frac{\pi(X)}{1-\pi(X)}$$

Ce qui correspond à la cote d’un événement, la probabilité que l’événement se produise par rapport à la probabilité qu’il ne se produise pas. 

Ainsi : 

\begin{eqnarray}
\mathbb{P}(Y=1|X) &=& \pi(X) &=& logit^{-1}(\beta X) \\
logit(\mathbb{P}(Y=1|X)) &=& logit(\pi(X)) &=& \beta X \\
&&log(\frac{\pi(x)}{1-\pi(x)}) &=& \beta X \\
&&log(Odds(X)) &=& \beta X \\
&&Odds(X) &=& e^{\beta X}

\end{eqnarray}

Notons au passage que $logit(\mathbb{P}(Y=1|X)) = log(Odds(X))$ est une varible aléatoire modélisée par un modèle linéaire. 
C’est à partir de ce modèle linéaire que l’on obtient les p-valeurs du modèle logistique. La regression logistique est un modéle linéaire géneralisé (*glm*) qui utilise la fonction *logit* comme fontion de lien.


```{r echo=TRUE, results="verbatim"}
m = glm(d$Sex~d$Bwt, family = binomial(logit))
m$coefficients
summary(m)
```

On définit les **odds ratio** :

$$OR_{u/v} = \frac{odd(X = u)}{odd(X=v)} = e^{\beta (u-v)}$$

$OR$ estime par exemple le rapport malades/non-malades entre deux populations (facilement compréhensible quand $X$ est qualitatif).

$OR$ est directement calculable à partir des coefficients de la régression $\beta$, il permet d’interpréter les $\widehat{\beta_k}$ et de mesurer l‘effet de la variable $X_k$ sur le modèle.





On définit le **risque relatif** :

$$ RR_{u/v} = \frac{\pi(X = u)}{\pi(X=v)} = \frac{P(Y=1|X=u)}{P(Y=1|X=v)} $$

Note : si $p$ est petit alors $\frac{p}{1-p} \simeq p$ et l’*odds ratio* est proche du risque relatif.


$RR$ estime le risque d’être par exemple malade.

$OR$ et $RR$ donnent la même indication sur la relation entre $Y$ et $X$ :

1) Si $RR_{u/v}$ (ou $OR_{u/v}$) $>1$ alors il y a plus de risque de $Y=1$ si $X=u$ que si $X=v$
2) Si $RR_{u/v}$ (ou $OR_{u/v}$) $<1$ alors il y a moins de risque de $Y=1$ si $X=u$ que si $X=v$
3) Si $RR_{u/v}$ (ou $OR_{u/v}$) $=1$ alors $Y$ n’est pas influencée par $X=u$ vs. $X=v$ (i.e. Y indépendant des catégories $u$ et $v$ de $X$)


















